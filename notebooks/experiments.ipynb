{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import torch\n",
    "from pathlib import Path, PosixPath\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from scr.readers import *\n",
    "from scr.download_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = get_raw_documents(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepvk/USER-bge-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 512\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=' ', \n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=int(CHUNK_SIZE / 5),\n",
    "    strip_whitespace=True, \n",
    "    length_function=lambda x: len(tokenizer.encode(x, add_special_tokens=False))\n",
    ")\n",
    "\n",
    "chunked_documents_with_page_content = text_splitter.split_documents(raw_documents)\n",
    "print(len(chunked_documents_with_page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_text_splitters.character.RecursiveCharacterTextSplitter"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_documents = [doc.page_content for doc in chunked_documents_with_page_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"deepvk/USER-bge-m3\",\n",
    "    model_kwargs={'device': device})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_texts(chunked_documents, embedding_model, distance_strategy=DistanceStrategy.COSINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(folder_path=\"../data/vector_dbs\", index_name=\"faiss_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_retrieve = 5\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": num_docs_retrieve})\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    chunked_documents_with_page_content)\n",
    "bm25_retriever.k = num_docs_retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_database = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_rerank = 3\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=num_docs_rerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vector_database\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = compression_retriever.invoke('революция')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(doc: Document) -> dict:\n",
    "    \"\"\"Редактирование документа, полученного из ретривера\n",
    "\n",
    "    Args:\n",
    "        doc (Document): документ из ретривера\n",
    "\n",
    "    Returns:\n",
    "        dict: словарь с содержанием 'содержание' и названием источника 'источник'\n",
    "    \"\"\"\n",
    "    page_content = doc.page_content\n",
    "    source = doc.metadata['source'].split(\"/\")[-1].split(\".\")[0]\n",
    "    return {'источник': source, 'содержание': page_content}\n",
    "\n",
    "def process_retrieve_output(docs: list[Document]) -> list[dict]:\n",
    "    \"\"\"Функция обработки документов из ретривера.\n",
    "\n",
    "    Args:\n",
    "        docs (list[Document]): документы из ретривера\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: обработанные документы\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for doc in docs:\n",
    "        processed_doc = process_data(doc)\n",
    "        processed_documents.append(processed_doc)\n",
    "    return processed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_documents = process_retrieve_output(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "A non-annotated attribute was detected: `retries = 3`. All model fields require a type annotation; if `retries` is not meant to be a field, you may be able to resolve this error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/model-field-missing-annotation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myandex_chain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YandexLLM\n",
      "File \u001b[0;32m~/Desktop/PETs/Музей/.venv/lib/python3.10/site-packages/yandex_chain/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.0.10\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mYandexGPTEmbeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YandexEmbeddings\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mYandexGPT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YandexLLM, YandexGPTModel\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mYandexGPTClassifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YandexGPTClassifier\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChatYandexGPT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatYandexGPT\n",
      "File \u001b[0;32m~/Desktop/PETs/Музей/.venv/lib/python3.10/site-packages/yandex_chain/YandexGPT.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m     Pro32k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     18\u001b[0m     Custom \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m99\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mYandexLLM\u001b[39;00m(LLM):\n\u001b[1;32m     21\u001b[0m     temperature : \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     22\u001b[0m     max_tokens : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1500\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/PETs/Музей/.venv/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py:115\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m config_wrapper \u001b[38;5;241m=\u001b[39m ConfigWrapper\u001b[38;5;241m.\u001b[39mfor_model(bases, namespace, kwargs)\n\u001b[1;32m    114\u001b[0m namespace[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m config_wrapper\u001b[38;5;241m.\u001b[39mconfig_dict\n\u001b[0;32m--> 115\u001b[0m private_attributes \u001b[38;5;241m=\u001b[39m \u001b[43minspect_namespace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignored_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_field_names\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m private_attributes \u001b[38;5;129;01mor\u001b[39;00m base_private_attributes:\n\u001b[1;32m    119\u001b[0m     original_model_post_init \u001b[38;5;241m=\u001b[39m get_model_post_init(namespace, bases)\n",
      "File \u001b[0;32m~/Desktop/PETs/Музей/.venv/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py:428\u001b[0m, in \u001b[0;36minspect_namespace\u001b[0;34m(namespace, ignored_types, base_class_vars, base_class_fields)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m    425\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m requires a type annotation\u001b[39m\u001b[38;5;124m'\u001b[39m, code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel-field-missing-annotation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    426\u001b[0m             )\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m    429\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA non-annotated attribute was detected: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m`. All model fields require a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    430\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype annotation; if `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not meant to be a field, you may be able to resolve this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    431\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror by annotating it as a `ClassVar` or updating `model_config[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignored_types\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    432\u001b[0m                 code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel-field-missing-annotation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    433\u001b[0m             )\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ann_name, ann_type \u001b[38;5;129;01min\u001b[39;00m raw_annotations\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    437\u001b[0m         is_valid_privateattr_name(ann_name)\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m ann_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m private_attributes\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ann_type, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    445\u001b[0m     ):\n",
      "\u001b[0;31mPydanticUserError\u001b[0m: A non-annotated attribute was detected: `retries = 3`. All model fields require a type annotation; if `retries` is not meant to be a field, you may be able to resolve this error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/model-field-missing-annotation"
     ]
    }
   ],
   "source": [
    "from yandex_chain import YandexLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "YGPTFOLDERIP = os.getenv('YGPT_folder_ip')\n",
    "YGPTAPIKEY = os.getenv('YGPT_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YandexLLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mYandexLLM\u001b[49m(folder_id\u001b[38;5;241m=\u001b[39mYGPTFOLDERIP, \n\u001b[1;32m      2\u001b[0m                 api_key\u001b[38;5;241m=\u001b[39mYGPTAPIKEY,\n\u001b[1;32m      3\u001b[0m                 model\u001b[38;5;241m=\u001b[39mYandexGPTModel\u001b[38;5;241m.\u001b[39mPro)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YandexLLM' is not defined"
     ]
    }
   ],
   "source": [
    "llm = YandexLLM(folder_id=YGPTFOLDERIP, \n",
    "                api_key=YGPTAPIKEY,\n",
    "                model=YandexGPTModel.Pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "museum_voice_assistant",
   "language": "python",
   "name": "museum_voice_assistant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
